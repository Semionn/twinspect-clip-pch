# Datasets for ISCC Evaluation

## Introduction

### Dataset Types

To test the capabilities of the ISCC, we need a collection of test data, including ground truth
data. Ground truth data are essentially benchmarks that provide us with a standard of truth. They
help us measure the ISCC's accuracy in detecting similar or duplicate content. These benchmarks can
be established in two ways:

1. **Real-world media file collections** annotated with information about near-duplicates within the
   dataset. These collections offer insights into the ISCC's performance in a real-world setting,
   albeit contingent on the quality of annotations.
1. **Synthetically transformed media files**. We can take a unique media file and apply various
   modifications to it. This strategy tests the ISCC's resilience against different transformations,
   although the synthetic changes might not fully mirror real-world variations.

### Data Folders

To streamline processing and ensure comparability, our datasets adhere to a predefined directory
structure and file naming convention. This approach implicitly encodes ground truth and optionally
transformation information, simplifying the evaluation of an algorithm's performance against
different datasets. Here's an example snapshot of our data folder structure:

```bash
data-folder
├── cluster1           # Collections of media files considered duplicates
│   ├── 0original.mp3  # The first file is the original (lexicographic porder)
│   ├── mod1.mp3       # Variation that should match against other files in the cluster
│   ├── mod2_rot20.mp3 # Variation with named transform
│   └── ...            # Create as many modified versions as you like
├── cluster2           # A cluster folder can have any name
│   ├── file1.mp3      # Files in cluster folders can have any name
│   └── ...            # A cluster folder may have only one file (query with no match)
├── sample1.mp3        # Top-Level files that should NOT match against any other files
├── sample2.mp3        # The ratio of distractor content vs. cluster content is relevant for metrics
└── ...
```

## Evaluation Datasets

!!! info
    The content of this section is autogenerated based on latest published configuration of the
    TwinSpect Benchmark.
