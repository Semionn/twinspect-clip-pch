# Datasets for ISCC Evaluation

## Introduction

### Dataset Types

To test the capabilities of the ISCC, we need a collection of test data, including ground truth
data. Ground truth data are essentially benchmarks that provide us with a standard of truth. They
help us measure the ISCC's accuracy in detecting similar or duplicate content. These benchmarks can
be established in two ways:

1. **Real-world media file collections** annotated with information about near-duplicates within the
   dataset. These collections offer insights into the ISCC's performance in a real-world setting,
   albeit contingent on the quality of annotations.
1. **Synthetically transformed media files**. We can take a unique media file and apply various
   modifications to it. This strategy tests the ISCC's resilience against different transformations,
   although the synthetic changes might not fully mirror real-world variations.

### Data Folders

To streamline processing and ensure comparability, our datasets adhere to a predefined directory
structure and file naming convention. This approach implicitly encodes ground truth and optionally
transformation information, simplifying the evaluation of an algorithm's performance against
different datasets. Here's an example snapshot of our data folder structure:

```bash
data-folder
├── cluster1           # Collections of media files considered duplicates
│   ├── 0original.mp3  # The first file is the original (lexicographic porder)
│   ├── mod1.mp3       # Variation that should match against other files in the cluster
│   ├── mod2_rot20.mp3 # Variation with named transform
│   └── ...            # Create as many modified versions as you like
├── cluster2           # A cluster folder can have any name
│   ├── file1.mp3      # Files in cluster folders can have any name
│   └── ...            # A cluster folder may have only one file (query with no match)
├── sample1.mp3        # Top-Level files that should NOT match against any other files
├── sample2.mp3        # The ratio of distractor content vs. cluster content is relevant for metrics
└── ...
```

## Evaluation Datasets

!!! info
    The content of this section is autogenerated based on latest published configuration of the
    TwinSpect Benchmark.

______________________________________________________________________

### ISCC-FMA-10K

!!! abstract inline end "Dataset Info"
    - **ID**: 142e3bd331044320
    - **Mode**: Audio
    - **Size**: 69.0 GB
    - **Files**: 10000

The **ISCC-FMA-10K** is a benchmark dataset, designed to assess the accuracy of audio identification
algorithms. It includes ground truth data for a total of **10000 audio files** with near-duplicates
organized into **500 clusters**.

Additionally, the dataset contains **4500 unique** audio files, with no corresponding duplicates
within the set.

??? note "Clustering Details"
    Each cluster contains **11 near-duplicate** audio files.

??? note "Synthetic Transformations"
    The following transformations were applied to **500 files** of the dataset to simulate different
    conditions that might be encountered in real-world applications:

    - compress-medium
    - transcode-mp3-128kbps
    - fade-8s-both
    - equalize
    - transcode-ogg-64kbps
    - loudnorm
    - echo
    - transcode-aac-32kbps
    - trim-1s-both
    - trim-5s-both

______________________________________________________________________

### MIRFLICKR-MFND

!!! abstract inline end "Dataset Info"
    - **ID**: 99107a584e0bd4e3
    - **Mode**: Image
    - **Size**: 999.5 MB
    - **Files**: 7942

The **MIRFLICKR-MFND** is a benchmark dataset, designed to assess the accuracy of image
identification algorithms. It includes ground truth data for a total of **7942 image files** with
near-duplicates organized into **3825 clusters**.

??? note "Clustering Details"
    Clusters contain an average of **2.08 near-duplicate** image files.

    **Cluster sizes**

    - **Minimum**: 1
    - **Maximum**: 14
    - **Mean**: 2.08
    - **Median**: 2.0
